# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FYtQNojrj7pwSqK8aucyE4stBrFeUOjn

## Processing
"""

# Install bcftools
!apt-get install -y bcftools

# Install cyvcf2 and its dependencies
!pip install cyvcf2

import pandas as pd

# Load T2D SNPs file
gwas_df = pd.read_csv("t2d_snp.txt", sep=r"\s+", engine="python")

# Clean column names (strip leading/trailing spaces)
gwas_df.columns = gwas_df.columns.str.strip()

# Sort by p-value (most significant first)
df_sorted = gwas_df.sort_values(by="P_VALUE")

# Take top 100,000
df_top100k = df_sorted.head(100000)

df_top100k.to_csv("t2d_snp_top100k.txt", sep="\t", index=False)

with open("t2d_snps.bed", "w") as f:
    for _, row in df_top100k.iterrows():
        chrom = str(row['CHROMOSOME']).strip()
        pos = int(row['POSITION'])
        rsid = row['SNP'].strip()
        # BED is 0-based, half-open
        f.write(f"chr{chrom}\t{pos}\t{pos+1}\t{rsid}\n")

!awk -v OFS='\t' '{mid=int(($2+$3)/2); print $1, mid-50, mid+51, $4}' t2d_snps.bed > t2d_snp_expanded.bed

!sed 's/^chr//' t2d_snp_expanded.bed > t2d_snps_nochr_expanded.bed

!apt-get install -y bedtools

!wget http://hgdownload.cse.ucsc.edu/goldenPath/hg18/bigZips/hg18.fa.gz

# Unzip
!gunzip hg18.fa.gz

!apt-get update
!apt-get install -y samtools

!samtools faidx hg18.fa

!bedtools getfasta -fi hg18.fa -bed t2d_snp_expanded.bed -fo snps_contexts_101bp.fa

!pip install biopython
from Bio import SeqIO
def write_fasta_no_wrap(records, out_path):
    with open(out_path, "w") as out_f:
        for record in records:
            out_f.write(f">{record.id}\n{str(record.seq)}\n")
records = []
for record in SeqIO.parse("snps_contexts_101bp.fa", "fasta"):
    record.seq = record.seq.upper()
    records.append(record)

write_fasta_no_wrap(records, "snps_contexts_101bp_upper.fa")

import gzip
import os
from Bio.SeqRecord import SeqRecord

#Load the risk alleles
df = pd.read_csv("t2d_snp_top100k.txt", sep="\t")
print(f"Loaded {len(df)} SNPs from t2d_snp_y=top100k.txt")

# Build SNP to risk allele dictionary
snp_risk_dict = {
    f"chr{row['CHROMOSOME']}:{row['POSITION']}": row['RISK_ALLELE']
    for _, row in df.iterrows()
    if row['RISK_ALLELE'] in "ACGT"
}

#Load reference FASTA and create alternate sequences
input_fasta = "snps_contexts_101bp_upper.fa"
output_fasta = "snps_contexts_alt_101bp.fa"
snp_index = 50

records = []

for record in SeqIO.parse(input_fasta, "fasta"):
    header = record.id
    seq = str(record.seq)
    chrom, coords = header.split(":")
    start, _ = map(int, coords.split("-"))
    snp_pos = start + snp_index  # 1-based position
    key = f"{chrom}:{snp_pos}"

    alt = snp_risk_dict.get(key)
    if alt:
        new_seq = seq[:snp_index] + alt + seq[snp_index+1:]
        new_record = SeqRecord(
            seq=new_seq,
            id=header + f"|ALT={alt}",
            description=""
        )
        records.append(new_record)
        print(f"Replaced SNP at {key} with risk allele {alt}")
    else:
        print(f"No risk allele found for {key}, skipping")

#Write to output FASTA

write_fasta_no_wrap(records, output_fasta)
print(f"\n Saved {len(records)} SNP-altered sequences to {output_fasta}")

"""## Alternate Attention"""

!pip install transformers torch

!git clone https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted

with open("DNABERT2-AttentionExtracted/flash_attn_triton.py", "w") as f:
    f.write("""
# Patched to disable FlashAttention
flash_attn_qkvpacked_func = None
""")

from transformers import AutoTokenizer, AutoModel
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained("DNABERT2-AttentionExtracted", trust_remote_code=True)
model = AutoModel.from_pretrained("DNABERT2-AttentionExtracted", trust_remote_code=True, output_attentions = True).to(device)

!pip install biopython
from Bio import SeqIO

fasta_path = "snps_contexts_alt_101bp.fa"
records = list(SeqIO.parse(fasta_path, "fasta"))

seq_ids = [rec.id for rec in records]
sequences = [str(rec.seq).upper() for rec in records]

batch_size = 32
attention_results = []
save_every = 1000
processed = 0

def batch_tokenize(seqs):
    return tokenizer(seqs, return_tensors="pt", padding=True, truncation=True)

for i in range(0, len(sequences), batch_size):
    batch_seqs = sequences[i:i+batch_size]
    batch_ids = seq_ids[i:i+batch_size]

    inputs = batch_tokenize(batch_seqs)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs, return_dict=True, output_attentions=True)

    attentions = outputs[3]
    last_layer_attention = attentions[-1]

    for j, seq_id in enumerate(batch_ids):
        attention_results.append({
            "id": seq_id,
            "sequence": batch_seqs[j],
            "attention": last_layer_attention[j].cpu()
        })

    processed += len(batch_seqs)

    if processed % save_every == 0:
        torch.save(attention_results, f'attention_results_upto_{processed}.pt')
        print(f'Saved attention results for {processed} sequences.')

# Save final results
torch.save(attention_results, 'attention_results_final.pt')
print('Saved final attention results.')

import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns

attention_results_upto_60000 = torch.load('attention_results_upto_60000.pt')

max_len = max(att["attention"].shape[-1] for att in attention_results_upto_60000)

max_length = 26

def pad_attention(att):
    pad_amt = max_length - att.shape[-1]
    return F.pad(att, (0, pad_amt, 0, pad_amt))


all_attn = torch.stack([pad_attention(x["attention"]) for x in attention_results_upto_60000])

# Average over sequences
avg_attn = all_attn.mean(dim=0)

snp_token_index = 10

# Attention TO the SNP position (column)
attn_to_snp = avg_attn[:, :, snp_token_index]

# Attention FROM the SNP position (row)
attn_from_snp = avg_attn[:, snp_token_index, :]

num_heads = avg_attn.shape[0]
fig, axs = plt.subplots(nrows=num_heads, figsize=(8, 2 * num_heads))

for i in range(num_heads):
    axs[i].plot(attn_to_snp[i].cpu().numpy())
    axs[i].set_title(f"Head {i} – Attention to SNP")
    axs[i].set_ylabel("Score")
    axs[i].set_xlabel("Token position")
    axs[i].axvline(snp_token_index, color='r', linestyle='--', label="SNP")
    axs[i].legend()

plt.tight_layout()
plt.show()

fig, axs = plt.subplots(nrows=num_heads, figsize=(8, 2 * num_heads))

for i in range(num_heads):
    axs[i].plot(attn_from_snp[i].cpu().numpy())
    axs[i].set_title(f"Head {i} – Attention from SNP")
    axs[i].set_ylabel("Score")
    axs[i].set_xlabel("Token position")
    axs[i].axvline(snp_token_index, color='r', linestyle='--', label="SNP")
    axs[i].legend()

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

attn_from_snp = all_attn[:, :, snp_token_index, :]
attn_to_snp = all_attn[:, :, :, snp_token_index]
mean_from_snp = attn_from_snp.mean(dim=0)
mean_to_snp = attn_to_snp.mean(dim=0)


max_length = 26

mean_from_snp_np = mean_from_snp[:, :max_length].cpu().numpy()
mean_to_snp_np = mean_to_snp[:, :max_length].cpu().numpy()


width_per_token = 0.6
fig_width = max(8, max_length * width_per_token)
fig_height = 10

# Plot heatmaps
fig, axes = plt.subplots(2, 1, figsize=(fig_width, fig_height))

sns.heatmap(mean_from_snp_np, ax=axes[0], cmap='coolwarm', cbar=True)
axes[0].set_title("Attention FROM SNP position to other positions")
axes[0].set_ylabel("Head")
axes[0].set_xlabel("Token")
axes[0].set_xticks(np.arange(max_length) + 0.5)
axes[0].set_xticklabels([str(i) for i in range(max_length)], rotation=90)
axes[0].set_aspect('auto')

sns.heatmap(mean_to_snp_np, ax=axes[1], cmap='coolwarm', cbar=True)
axes[1].set_title("Attention TO SNP position from other positions")
axes[1].set_ylabel("Head")
axes[1].set_xlabel("Token")
axes[1].set_xticks(np.arange(max_length) + 0.5)
axes[1].set_xticklabels([str(i) for i in range(max_length)], rotation=90)
axes[1].set_aspect('auto')

plt.tight_layout()
plt.show()

head_vectors = mean_to_snp[:, :max_length].cpu().numpy()
pca = PCA(n_components=2)
head_vectors_2d = pca.fit_transform(head_vectors)
n_clusters = 4
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
labels = kmeans.fit_predict(head_vectors_2d)
plt.figure(figsize=(8, 6))
plt.scatter(head_vectors_2d[:, 0], head_vectors_2d[:, 1], c=labels, cmap='tab10', s=100)
for i in range(len(labels)):
    plt.text(head_vectors_2d[i, 0], head_vectors_2d[i, 1], str(i), fontsize=12, ha='center', va='center', color='white')
plt.title("Clustered Attention Heads To SNP")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.grid(True)
plt.show()

cluster_order = np.argsort(labels)  # sort heads by cluster
ax = sns.heatmap(mean_to_snp[cluster_order, :max_length].cpu().numpy(), cmap="coolwarm", xticklabels=True, yticklabels=cluster_order)
ax.set_title("Head Attention Patterns To SNP Grouped by Cluster")
ax.set_xlabel("Token Position")
ax.set_xticklabels([str(i) for i in range(max_length)], rotation=90)
ax.set_ylabel("Head (Clustered)")
ax.set_xticks(np.arange(max_length) + 0.5)
ax.set_xticklabels([str(i) for i in range(max_length)], rotation=90)
plt.tight_layout()
plt.show()

head_vectors = mean_from_snp[:, :max_length].cpu().numpy()
pca = PCA(n_components=2)
head_vectors_2d = pca.fit_transform(head_vectors)
n_clusters = 4
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
labels = kmeans.fit_predict(head_vectors_2d)
plt.figure(figsize=(8, 6))
plt.scatter(head_vectors_2d[:, 0], head_vectors_2d[:, 1], c=labels, cmap='tab10', s=100)
for i in range(len(labels)):
    plt.text(head_vectors_2d[i, 0], head_vectors_2d[i, 1], str(i), fontsize=12, ha='center', va='center', color='white')
plt.title("Clustered Attention Heads From SNP")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.grid(True)
plt.show()

cluster_order = np.argsort(labels)  # sort heads by cluster
ax = sns.heatmap(mean_from_snp[cluster_order, :max_length].cpu().numpy(), cmap="coolwarm", xticklabels=True, yticklabels=cluster_order)
ax.set_title("Head Attention Patterns From SNP Grouped by Cluster")
ax.set_xlabel("Token Position")
ax.set_xticklabels([str(i) for i in range(max_length)], rotation=90)
ax.set_ylabel("Head (Clustered)")
ax.set_xticks(np.arange(max_length) + 0.5)
ax.set_xticklabels([str(i) for i in range(max_length)], rotation=90)
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

head_vectors = avg_attn.reshape(avg_attn.shape[0], -1).cpu().numpy()
pca = PCA(n_components=2).fit_transform(head_vectors)
kmeans = KMeans(n_clusters=4).fit(pca)

for i, txt in enumerate(range(len(pca))):
    plt.annotate(txt, (pca[i,0], pca[i,1]))

# Plot
import matplotlib.pyplot as plt
plt.scatter(pca[:,0], pca[:,1], c=kmeans.labels_)
plt.title("Head Specialization Clusters")

def find_snp_token_idx(sequence, snp_pos=50, tokenizer=None):
    """
    Given a DNA sequence and SNP position (0-based),
    find which BPE token index covers the SNP base.

    Args:
      sequence (str): DNA sequence
      snp_pos (int): SNP base index in sequence
      tokenizer: HuggingFace tokenizer (DNABERT2)

    Returns:
      snp_token_idx (int): index of token covering SNP base
      tokens (list[str]): list of tokens for sequence
    """

    tokens = tokenizer.tokenize(sequence)
    pos = 0
    token_spans = []
    for t in tokens:
        token_str = t.replace("##", "")
        length = len(token_str)
        token_spans.append((pos, pos + length))
        pos += length

    reconstructed = "".join([t.replace("##", "") for t in tokens])
    assert reconstructed == sequence,
    # Find token covering SNP position
    for idx, (start, end) in enumerate(token_spans):
        if start <= snp_pos < end:
            return idx, tokens

    # If SNP pos not found
    return None, tokens

"""## Reference Attention"""

# Load sequences from your SNP alt FASTA file
fasta_path = "snps_contexts_101bp_upper.fa"
records = list(SeqIO.parse(fasta_path, "fasta"))

seq_ids = [rec.id for rec in records]
sequences = [str(rec.seq).upper() for rec in records]

batch_size = 32
refattention_results = []
save_every = 1000
processed = 0

def batch_tokenize(seqs):
    return tokenizer(seqs, return_tensors="pt", padding=True, truncation=True)

for i in range(0, len(sequences), batch_size):
    batch_seqs = sequences[i:i+batch_size]
    batch_ids = seq_ids[i:i+batch_size]

    inputs = batch_tokenize(batch_seqs)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs, return_dict=True, output_attentions=True)

    attentions = outputs[3]
    last_layer_attention = attentions[-1]

    for j, seq_id in enumerate(batch_ids):
        refattention_results.append({
            "id": seq_id,
            "sequence": batch_seqs[j],
            "attention": last_layer_attention[j].cpu()
        })

    processed += len(batch_seqs)

    if processed % save_every == 0:
        torch.save(refattention_results, f'refattention_results_upto_{processed}.pt')
        print(f'Saved attention results for {processed} sequences.')

# Save final results
torch.save(refattention_results, 'refattention_results_final.pt')
print('Saved final attention results.')

refattention_results_upto_60000 = torch.load('refattention_results_upto_60000.pt')

ref_all_attn = torch.stack([pad_attention(x["attention"]) for x in refattention_results_upto_60000])

# Average over sequences
ref_avg_attn = ref_all_attn.mean(dim=0)

snp_token_index = 10

diff_attn = avg_attn - ref_avg_attn

from_diff = diff_attn[:, snp_token_index, :]
to_diff = diff_attn[:, :, snp_token_index]

sns.heatmap(from_diff.cpu().numpy(), cmap="coolwarm", center=0)
plt.title("Change in Attention FROM SNP (Alt - Ref)")
plt.xlabel("Token Position")
plt.ylabel("Head")
plt.show()

sns.heatmap(to_diff.cpu().numpy(), cmap="coolwarm", center=0)
plt.title("Change in Attention TO SNP (Alt - Ref)")
plt.xlabel("Token Position")
plt.ylabel("Head")
plt.show()

head_mean_abs_change = torch.mean(torch.abs(from_diff), dim=1)
print(head_mean_abs_change)

head_max_abs_change = torch.max(torch.abs(from_diff), dim=1).values
print(head_max_abs_change)

token_mean_change = torch.mean(from_diff, dim=0)
print(token_mean_change)

import torch
token_mean_change = torch.tensor([-2.2941e-04, -5.9519e-05, -3.6798e-05, -4.1871e-05, -4.2690e-05,
        -3.5576e-05, -4.6471e-05, -7.4593e-05, -7.4001e-05,  1.1403e-04,
        -1.4521e-04,  1.4308e-04, -7.4813e-05, -1.0280e-04, -3.3915e-05,
        -4.9710e-05, -3.1303e-05,  2.2928e-05, -1.4249e-04, -1.6990e-04,
        -1.6759e-04, -2.0615e-04,  8.0327e-04,  1.8937e-04,  4.4878e-04,
         7.5864e-05])

token_mean_change.argsort(dim=0, descending=True)[:5]

max_change_pos = torch.argmax(torch.abs(token_mean_change))
print(token_mean_change)